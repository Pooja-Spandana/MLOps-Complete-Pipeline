Building Pipeline:
1> Create a GitHub repo and clone it in local (Add experiments).
2> Add src folder along with all components(run them individually).
3> Add data, models, reports directories to .gitignore file
4> Now git add, commit, push

Setting up DVC pipeline (without params):
5> Create dvc.ymal file & add stages to it.
6> dvc init, then do dvc repro to test the pipeline automation.
    check dvc dag -> directed acyclic graph, it shows the flow in which each component is connected to each other
7> now git add, commit, push.

Setting up DVC pipeline (with params):
8> add params.yaml file
9> add the params setup
10> Do dvc repro again to test the pipeline aling with params
11> now git add, commit, push

Expermients with DVC:
12> pip install dvclive
13> Add the dvclive code block 
14> Do "dvc exp run", it will create a new dvc.yaml (if already not there) and dvclive directory (each run will be considered as an experiment by DVC)
15> Do "dvc exp show" on terminal to see the experiments or use extension on VSCode (install dvc extension)
16> Do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
17> Change params, re-run code (produce new experiments)
18> Now git add, commit, push

Adding a remote S3 storage to DVC
19> Login to AWS console
20> Create an IAM user
21> Create S3 (enter unique name & create)
22> pip install dvc[s3]
23> pip install awscli
24> aws configure
25> dvc remote add -d dvcstore s3://bucketname
26> dvc commit -push the exp outcome that you want to keep
27> now git add, commit, push